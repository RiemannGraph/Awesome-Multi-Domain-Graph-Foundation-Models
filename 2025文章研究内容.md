| 论文| MetaInfo (会议/作者) | 解决的核心问题  | 模型框架/基础 | 核心创新点 |
| :--- | :--- | :--- | :--- | :--- |
| **SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation** | WWW 2025<br>Xingtong Yu, Zechuan Gong, Chang Zhou, Yuan Fang, Hui Zhang | 针对**无文本属性图**，解决多域预训练与跨域适应面对的挑战。传统方法依赖文本描述，这篇专注于结构信息。 | **基于通用的消息传递图神经网络 (Message-passing GNN)**。 | 1. 构造了**结构令牌**来对齐不同图域的拓扑，无需文本。<br>2. 设计了**双提示适应框架**，将统一结构知识与域特异性信息解耦整合，实现高效跨域迁移。 |
| **RiemannGFM: Learning a Graph Foundation Model from Riemannian Geometry** | WWW 2025 (Oral)<br>Li Sun, Zhenhao Huang, Suyang Zhou, Qiqi Wan, Hao Peng, Philip Yu | 解决现有基于LLM的GFM的两大局限：(1) 仅限于**文本属性图**，无法处理更广泛的**非文本图**；(2) 为适配LLM而将图序列化的方法**损失了复杂结构信息**。旨在预训练一个不依赖文本、真正学习通用结构知识的模型。 | **提出了黎曼几何预训练框架**。 | 1. **发现并形式化了“树与环”这一最小完备的结构词汇**，为图的通用表示提供了离散化基元。<br>2. **设计并构建了双曲-球面产品丛**作为该词汇的表示空间。<br>3. 实现了**完全非文本、纯几何拓扑驱动**的预训练框架。|
| **Unified Graph Neural Networks Pre-training for Multi-domain Graphs (MDP-GNN)** | AAAI 2025<br>Mingkai Lin, Xiaobin Hong, Wenzhong Li, Sanglu Lu | 打破“**一域一模型**”的限制，发现连接不同图域的潜在**元域**，利用它来预训练一个统一模型，以提升在数据稀缺的下游任务上的性能。 | **提出MDP-GNN框架**。| 1. 构造了**域合成网络**这一模块，通过双层优化自动挖掘不同图域间的潜在联系（元域）。<br>2. 将节点特征语义整合、元域发现与基于Wasserstein距离的分布对齐**整合进一个统一的框架**，实现了跨图域的知识统一与迁移。 |
| **GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs** | WWW 2025<br>Yun Zhu, Haizhou Shi, Xiaotang Wang, Yongchao Liu, Yaoke Wang, Boci Peng, Chuntao Hong, Siliang Tang | 解决**文本属性图基础模型**的跨域（零样本/少样本）迁移能力弱的问题。 | **自监督对比学习框架**。利用LLM生成图-摘要对数据进行预训练。 | 1. 构造了**由LLM生成的大规模“图-摘要”对数据集**，作为模型预训练语料，解决了高质量对齐数据稀缺的问题。<br>2. 提出了**结合不变学习的对比学习预训练方法**，使模型学习域不变特征，显著提升**零样本迁移**的鲁棒性。<br>3. 提出了**与预训练目标对齐的图提示调优技术**，用于少样本迁移，以缓解灾难性遗忘并降低学习成本。 |
| **SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs** | NeurIPS 2025<br>Ruyue Liu, Rong Yin, Xiangzhen Bo, Xiaoshuai Hao, Yong Liu, Jinwen Zhong, Can Ma, Weiping Wang | 解决**文本属性图在跨域迁移时**面临的挑战。 | **双知识蒸馏框架**。利用文本作为统一表示媒介，弥合LLM的语义推理与GNN的结构建模之间的鸿沟。 | 1. 构造了**双知识蒸馏框架**，将**LLM（语义）与GNN（结构）协同蒸馏**到一个轻量的**结构感知多层感知机（MLP）** 中。<br>2. 引入了**内存机制**，构建一个**内存库**来存储和重用从不同领域学到的典型图表示（"不变"结构模式），以提升泛化能力。<br> |
| **GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation** | NeurIPS 2025<br>Linhao Luo, Zicheng Zhao, Gholamreza Haffari, Dinh Phung, Chen Gong, Shirui Pan | 解决**传统检索增强生成(RAG)**在复杂推理任务中的两大问题：(1) **难以建模知识片段间复杂关系**；(2) 基于图的RAG方法因**图结构的噪声与不完整性**而性能受限。 | **基于图神经网络(GNN)的推理框架**。在大规模知识图谱(60个图谱，1400万三元组)和文档(70万)上进行两阶段预训练。 | 1. 构造了一个**专为RAG任务定制的图基础模型(GFM)**，其核心是一个创新的、用于在图上进行关系推理的GNN。<br>2. 设计并实施了一个**大规模、两阶段（结构预训练 + 文档增强训练）的预训练流程**，使模型能同时学习通用图结构知识与特定任务知识。<br>3. 实现了**无需下游微调、开箱即用**的跨域检索能力。首个可直接应用于未见数据集的RAG专用图基础模型。 |
| **GOFA: A GENERATIVE ONE-FOR-ALL MODEL FOR JOINT GRAPH LANGUAGE MODELING** | ICLR 2025<br>Lecheng Kong, Jiarui Feng, Hao Liu, Chengsong Huang, Jiaxin Huang, Yixin Chen, Muhan Zhang | 解决现有图基础模型(GFM)难以同时兼顾**任务通用性**(如LLM)与**深度结构建模能力**(如GNN)的问题，构建一个能统一处理图与语言的生成式模型。 | **GNN与LLM交织的生成式架构**。将随机初始化的GNN层插入到**冻结的预训练LLM**中，通过多任务预训练联合优化。 | 1. 构造了一种**交织式模型架构**，让GNN层在LLM的每一层中处理结构信息，实现语义与结构的有机融合。<br>2. 明确定义GFM的三大属性(自监督预训练、任务流动性、图感知)，并设计**图级生成、问答、结构理解、信息检索**等多任务预训练目标来共同实现这些属性。<br>3. 通过**指令微调**获得零样本任务解决能力，在未见过的下游任务上验证了其泛化性。 |
| **GraphGPT: Generative Pre-trained Graph Eulerian Transformer** | ICML 2025<br>Qifang Zhao, Weidong Ren, Tianyu Li, Hong Liu, Xingsheng He, Xiaoxiao Xu | 构建**生成式预训练大模型**，以突破传统GNN和图Transformer在模型深度与规模上的限制，探索图基础模型的**缩放定律**，实现在大规模图基准上的高性能。 | **图欧拉变换器(GET)架构**。使用**欧拉路径**将图无损、可逆地线性化为节点-边-属性序列，然后用标准Transformer进行生成式预训练。 | 1. 构造了基于**欧拉路径的图序列化方法**，使标准Transformer能够高效、无损地处理图结构。<br>2. 成功将NLP中的**生成式预训练范式**（如下一令牌预测）应用于图数据，验证了其对图任务的有效性。<br>3. 将模型规模**成功扩展至20亿参数**，并在性能上持续受益，证明了图基础模型的可扩展性潜力。 |
| **AutoGFM: Automated Graph Foundation Model with Adaptive Architecture Customization** | ICML 2025 (Oral)<br>Haibo Chen, Xin Wang, Zeyang Zhang, Haoyang Li, Ling Feng, Wenwu Zhu | 解决现有图基础模型(GFM)依赖**固定架构**，无法为不同领域和任务定制最优架构，以及图神经架构搜索(GNAS)应用于GFM时存在的**架构不一致性**、**数据主导搜索**等核心问题。 | **图神经架构搜索(GNAS)框架**。核心包含三个模块：解耦对比图编码器（分离不变/可变模式）、不变性引导的架构定制策略、课程架构定制机制。 | 1. **首次将图神经架构搜索(GNAS)系统地引入GFM领域**，构建了一个自动化架构定制框架。<br>2. 提出了**不变性引导的架构定制策略**，基于学到的跨域不变模式指导定制过程，以应对最优架构随任务/域变化的核心挑战。<br>3. 设计了**课程架构定制机制**，动态调整搜索难度，有效缓解了特定数据主导搜索过程、影响最终模型泛化能力的问题。 |
| **GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation** | NeurIPS 2025<br>Zihao Guo, Qingyun Sun, Ziwei Zhang, Haonan Yuan, Huiping Zhuang, Xingcheng Fu, Jianxin Li | 解决图基础模型(GFM)在**持续学习多个新图领域**时发生的“灾难性遗忘”问题，特别是针对现有研究尚未探索的**跨图域的增量学习**这一新场景。 | **图域增量学习框架**。作为即插即用的插件，可接入任何预训练好的GFM。 | 1. 首次系统性地定义并解决了**图域增量学习**这一新问题，扩展了图持续学习的研究边界。<br>2. 构造了**偏差无知识保持**与**域感知分布判别**模块，分别从**稳定决策边界**和**处理未知域**两个角度应对灾难性遗忘。<br>3. 提出**域内与域间知识解耦**目标，结合参数高效微调，有效防止了不同域间嵌入表示的混淆与偏移。<br>4. 验证了该方法可作为通用插件与多种GFM无缝集成，展现了强大的应用潜力。 |
| **GRAVER: Generative Graph Vocabularies for Robust Graph Foundation Models Fine-tuning** | NeurIPS 2025<br>Haonan Yuan, Qingyun Sun, Junhua Shi, Xingcheng Fu, Bryan Hooi, Jianxin Li, Philip S. Yu | 解决GFM在**下游少样本微调**时，因**支持样本选择的随机性**和**预训练图与目标图间的结构差异**导致的**性能不稳定与波动**问题。实现鲁棒且高效的知识迁移。 | **生成式图词汇框架**。核心流程：1）通过自我图解耦分析提取可迁移的子图模式；2）利用基于Graphon的生成专家构建图词汇；3）通过**MoE-CoE网络**在微调时从词汇中路由知识。 | 1. 构造了**生成式图词汇**，将可迁移的、与类别相关的关键子图模式编码为离散的、可重用的知识单元。<br>2. 设计了**轻量级MoE-CoE路由网络**，在提示微调过程中动态、有选择地从词汇库中检索和整合最相关的知识，以适配具体任务。<br>3. 通过**理论分析与实证验证**相结合的方式，确保所提取子图模式的可迁移性，为方法提供了坚实支撑。 |
| **Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement** | NeurIPS 2025<br>Yinlin Zhu, Xunkai Li, Jishuo Jia, Miao Hu, Di Wu, Meikang Qiu | 在**数据孤岛（联邦学习）** 的约束下，训练一个能整合多客户端知识、并具备良好跨域迁移能力的图基础模型。 | **联邦图基础模型训练框架（FedGFM+）**。框架包含两大核心模块：**AncDAI**（基于锚点的域感知全局初始化）和**AdaDPP**（自适应域敏感本地提示池）。 | 1. 构造了**AncDAI模块**，通过在预训练前聚合各客户端的**域特定原型**作为语义锚点来初始化全局模型，**从理论上证明这些原型跨域可区分**，从而为解耦知识提供了强归纳偏置。<br>2. 构造了**AdaDPP模块**，在微调阶段形成一个**全局可学习的提示池**，使模型能自适应地从池中检索和集成来自不同域的相关知识，以增强目标图属性。<br>3. **首次系统地将联邦学习与图基础模型预训练深度整合**，提出了一种去中心化的GFM训练新范式，解决了知识纠缠的核心难题。 |
| **AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection** | KDD 2025<br>Hezhe Qiao, Chaoxi Niu, Ling Chen, Guansong Pang | 构建一个专门用于**跨图零样本/少样本异常检测**的基础模型，解决通用模型在此任务上泛化不佳的问题。 | **图无关原型学习框架**。模型在预训练中学习一组通用的、与数据无关的“正常”与“异常”**类原型**，并将节点表示与其邻居的**残差**与这些原型对齐。 | 1. 构造了**图无关的正常/异常类原型**作为可迁移的检测基准。其关键洞察是，要实现跨图有效检测，需要学习**与具体图数据无关的、判别性的**类别表征。<br>2. 创新性地引入并计算**节点表示残差**（节点与其邻居的表示偏差），将其作为与原型对齐的核心特征，从而将来自不同图的节点**投影到一个统一的特征空间**进行一致性评估。<br>3. 支持**零样本推理**与**少样本提示调优**两种模式，为不同资源条件的下游应用提供了灵活性。 |
| **Tokenphormer: Structure-aware Multi-token Graph Transformer for Node Classification** | AAAI 2025<br>Zijie Zhou, Zhaoqi Lu, Xuekai Wei, Rongqin Chen, Shenghui Zhang, Pak Lon Ip, Leong Hou U | 解决现有**图Transformer在节点分类任务**中，因依赖全局注意力而可能引入**无关节点噪声**与**丢失局部结构信息**的问题。增强对图局部与多粒度结构信息的感知能力。 | **多令牌图Transformer**。为每个节点生成多种**结构感知的令牌**（如Walk-token, SGPM-token, Hop-token）以表示不同粒度的结构信息，然后输入标准Transformer编码器进行协同学习。 | 1. 构造了多种**结构感知的令牌**：**Walk-token**（通过混合游走捕获结构与上下文）、**SGPM-token**（通过自监督图预训练模型确保全局信息）、**Hop-token**（扩展局部密度限制），**从不同角度与粒度捕获图的局部与全局信息**。<br>2. 将这种**细粒度的、多令牌的表示学习范式**成功引入图Transformer，通过将多种令牌协同输入Transformer，实现了对图结构**更细致、更灵活**的建模。 |
| **Enhanced Expert Merging for Mixture-of-Experts in Graph Foundation Models** | NeurIPS 2025<br>Lei Liu, Xingyu Xia, Qianqian Xie, Ben Liu, Wenjie Xu, Min Peng<br>Wuhan University | 解决基于混合专家（MoE）的图基础模型中，**路由器分配的最优专家性能不及次优/第三优专家**的问题；同时平衡多专家集成的性能优势与直接集成带来的**计算开销过大**、标准专家融合策略**性能不佳**的矛盾。 | **基于混合专家（MoE）的图基础模型架构** | 1. 通过实验发现MoE-GFM中**次优、第三优专家性能优于最优专家**的现象，为多专家知识融合提供了核心依据。<br>2. 提出两种增强型专家融合策略：(1) 基于知识蒸馏的融合方法，使参数融合后的专家行为与专家集成对齐；(2) 基于理论参数邻近性的方法，利用专家参数相似性近似集成输出，同时保留多样性。<br>3. 在保证计算效率的前提下，使模型性能接近专家集成的效果，兼顾了效率与性能。 |
| **GMoPE: A Prompt-Expert Mixture Framework for Graph Foundation Models** | -<br>Zhibin Wang, Zhixing Zhang, Shuqi Wang, Xuanting Xie, Zhao Kang | 解决图神经网络（GNN）在**跨不同领域和任务泛化时能力有限**的问题，以及现有方法存在的**负迁移、可扩展性不足、适配成本高**三大核心挑战。 | **混合专家（MoE）架构与图提示学习的融合框架** | 1. 提出将MoE架构与图提示学习无缝集成的框架，通过**专家特定提示向量**和**结构感知MoE路由**，使每个专家专注于不同子域并动态参与预测。<br>2. 引入**提示向量间的软正交约束**，促进专家多样性、防止专家崩溃，实现更均衡的专家利用率。<br>3. 采用**仅提示微调策略**，在迁移过程中显著降低时空复杂度，仅需少量适配开销就能达到接近全参数微调的性能。 |
| **SA²GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation** | AAAI 2026<br>Junhua Shi, Qingyun Sun, Haonan Yuan, Xingcheng Fu | 解决图基础模型（GFM）在**域噪声、结构扰动和对抗性攻击下鲁棒性不足**的问题，核心局限是对泛化至关重要的**层级结构语义建模不充分**，以及跨域适配中的负迁移问题。 | **结构感知语义增强的鲁棒GFM框架** | 1. 将基于熵的编码树转化为**结构感知文本提示**，编码层级结构先验以实现特征增强；通过自监督信息瓶颈机制，经结构引导压缩提炼鲁棒、可迁移的表示。<br>2. 提出**专家自适应路由机制**，结合混合专家（MoE）架构与空专家设计，有效缓解跨域适配中的负迁移问题。<br>3. 设计下游微调模块，通过**联合社区内与社区间结构学习**优化层级结构，提升适配效率。 |
| **Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks** | -<br>Ziyuan Tang, Jie Chen<br>arXiv提交时间：2025-06 | 解决**序列模型（Transformer）难以编码不同领域、不同大小图数据**的核心挑战，旨在为图构建类似GPT的基础模型，挖掘其处理图结构数据的涌现能力。 | **基于Transformer架构的图基础模型**，通过多样化图数据集预训练。 | 1. 提出将**节点表示为多个随机游走序列**，使Transformer可从序列中提取节点表示，进而构建边和图表示，适配不同规模和领域的图。<br>2. 为随机游走设计**新颖的上下文预测损失**，并从理论上分析其在区分邻域和图方面的表达能力。<br>3. 验证了模型预训练流程的有效性及下游任务适配性，为图结构数据的处理与推理提供了基础模型范式。 |
| **One Prompt Fits All: Universal Graph Adaptation for Pretrained Models** | NeurIPS 2025<br>Yongqi Huang, Jitao Zhao, Dongxiao He, Xiaobao Wang, Yawen Li, Yuxiao Huang, Di Jin, Zhiyong Feng | 解决图提示学习（GPL）的两大核心局限：(1) **机制共识缺失**（不同提示策略在模型输入层、层间、表示层介入，未明确提示与预训练模型的交互原理）；(2) **场景适配性有限**（难以泛化到多样下游场景，尤其数据分布偏移如“同质性-异质性图”迁移）。 | **通用图提示学习（GPL）框架（UniPrompt）** | 1. 理论分析现有GPL方法，揭示**表示层提示本质是微调简单下游分类器**，提出图提示学习应聚焦“释放预训练模型能力，让分类器适配下游场景”的核心思路。<br>2. 提出UniPrompt方法，可适配任意预训练模型，在**保留输入图完整性**的前提下充分发挥预训练模型潜力。<br>3. 实验验证该方法能有效集成多种预训练模型，在域内、跨域场景中均实现优异性能，达成“一个提示适配所有”的通用适配效果。 |
| **A Graph Laplacian Eigenvector-based Pre-training Method for Graph Neural Networks** | -<br>Howard Dai, Nyambura Njenga, Hiren Madhu, Siddharth Viswanath, Ryan Pellico, Ian Adelstein, Smita Krishnaswamy | 解决图基础模型（GFM）中**基于结构的预训练方法探索不足**的问题，以及传统消息传递GNN预训练时因**网络深度增加导致过平滑**、难以捕获全局和区域结构的核心挑战。 | **基于图拉普拉斯特征向量的预训练模块（LELM）** | 1. 提出LELM预训练模块，通过预测图拉普拉斯的**低频特征向量**，聚焦结构型预训练，弥补现有方法短板。<br>2. 设计新颖架构**克服过平滑问题**，使GNN能够学习长距离依赖关系。<br>3. 实证验证在下游分子性质预测任务中，经该框架预训练的模型性能优于基线模型。 |
| **Scalable Graph Generative Modeling via Substructure Sequences** | NeurIPS 2025<br>Zehong Wang, Zheyuan Zhang, Tianyi Ma, Chuxu Zhang, Yanfang Ye | 解决传统消息传递GNN的四大核心局限：**表达能力受限、过平滑、过挤压、长距离依赖建模不足**，以及模型/数据规模扩大时性能无法提升的**可扩展性难题**。 | **生成式图模式机（G²PM）框架**，基于Transformer的图生成式预训练架构 | 1. 突破消息传递范式，将图实例（节点、边、全图）表示为**子结构序列**，通过序列上的生成式预训练学习可泛化、可迁移的表示。<br>2. 具备极强可扩展性：在ogbn-arxiv基准上，模型规模扩展至6000万参数仍持续性能提升，优于300万参数后性能停滞的传统生成式方法。<br>3. 系统分析模型设计空间，明确支撑可扩展性与泛化性的关键架构选择，在节点/链路/图分类、迁移学习、跨图预训练等多任务中持续优于强基线。 |
| **UniGraph2: Learning a Unified Embedding Space to Bind Multimodal Graphs** | WWW 2025<br>Yufei He, Yuan Sui, Xiaoxin He, Yue Liu, Yifei Sun, Bryan Hooi | 解决两大核心局限：(1) 通用多模态基础模型（如CLIP）**忽视多模态数据中固有的图结构**；(2) 现有图基础模型主要聚焦文本属性图（TAGs），**无法处理多模态图（MMGs）的复杂性**（节点含多模态特征、边刻画实体关系）。 | **跨域图基础模型（UniGraph2）**，融合模态特定编码器、GNN与混合专家（MoE）的统一框架 | 1. 设计**模态特定编码器+GNN**架构，学习统一低维嵌入空间，同时捕获多模态信息与底层图结构。<br>2. 提出**大规模跨域多图预训练算法**，保障不同图域和模态间的有效迁移学习。<br>3. 引入**混合专家（MoE）组件**，对齐不同域和模态的特征，生成连贯鲁棒的统一嵌入。<br>4. 在多模态图表示学习、迁移学习、多模态生成等任务中显著优于SOTA，为MMGs提供可扩展、灵活的解决方案。 |
| **GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for Cross-Graph Transfer Learning** | KDD 2025<br>Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu | 解决预训练GNN的**跨图迁移能力弱**的核心问题，尤其忽视不同图数据集间的**特征与结构分布差异**，以及跨分布迁移时的灾难性遗忘、少量监督标签下适配性不足的挑战。 | **结构感知对比低秩适配框架（GraphLoRA）**，基于LoRA思想的参数高效迁移学习架构 | 1. 提出**结构感知最大均值差异（SMMD）**，对齐源图与目标图的节点特征分布差异。<br>2. 注入小型可训练GNN实现**低秩适配**，在弥合结构分布鸿沟的同时缓解灾难性遗忘。<br>3. 设计**结构感知正则化目标**，提升预训练GNN在少量监督标签目标图上的适配性。<br>4. 仅微调20%参数，在8个真实数据集上优于14个基线模型，即使在差异显著的图域间也表现优异。 |